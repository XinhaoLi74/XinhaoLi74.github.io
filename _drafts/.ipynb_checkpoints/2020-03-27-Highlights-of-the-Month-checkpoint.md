---
title:  "Highlights of the Month: January 2020"
date: 2020-03-30

tags:
  - Highlights of the Month
---

## Research Papers ðŸŽ“

[Molecule Attention Transformer](https://arxiv.org/abs/2002.08264). [[Code]](https://github.com/gmum/MAT/)
<p align='center'>
<img src="https://github.com/gmum/MAT/blob/master/assets/MAT.png" alt="architecture" width="600"/>
</p>

[A Primer in BERTology: What we know about how BERT works](https://arxiv.org/abs/2002.12327)


## Software and Tools ðŸ’» 

## Articles and Blog Posts ðŸ“ƒ

[Transformers are Graph Neural Networks](https://graphdeeplearning.github.io/post/transformers-are-gnns/)

[Graph Transformer tutorial](https://docs.dgl.ai/en/latest/tutorials/models/4_old_wines/7_transformer.html)

[From PyTorch to PyTorch Lightning â€” A gentle introduction](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09)
> PyTorch Lightning was created for professional researchers and PhD students working on AI research. PyTorch Lightning 

[WHAT IS TORCH.NN REALLY?](https://pytorch.org/tutorials/beginner/nn_tutorial.html)

[The Ultimate Guide to using the Python regex module](https://mlwhiz.com/blog/2019/09/01/regex/?utm_campaign=the-ultimate-guide-to-using-the-python-regex-module&utm_medium=social_link&utm_source=missinglettr-twitter)

[Tokenizers: How machines read](https://blog.floydhub.com/tokenization-nlp/)

[A Deep Dive into the Wonderful World of Preprocessing in NLP](http://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/)

[Machine Learning for Everyone](https://vas3k.com/blog/machine_learning/)

[From PyTorch to JAX: towards neural net frameworks that purify stateful code](https://sjmielke.com/jax-purify.htm)

## Notable Mentions âœ¨

[AI Curriculum](https://github.com/Machine-Learning-Tokyo/AI_Curriculum). Open Deep Learning and Reinforcement Learning lectures from top Universities like Stanford University, MIT, UC Berkeley.

[Huggingface's official notebook tutorials](https://github.com/huggingface/transformers/tree/master/notebooks)

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Today we&#39;re happy to release four new official notebook tutorials available in our documentation and in colab thanks to <a href="https://twitter.com/MorganFunto?ref_src=twsrc%5Etfw">@MorganFunto</a> to get started with tokenizers and transformer models in just seconds! <a href="https://t.co/zzBVWsEnef">https://t.co/zzBVWsEnef</a> (1/6) <a href="https://t.co/bpOYA6UTDk">pic.twitter.com/bpOYA6UTDk</a></p>&mdash; Hugging Face (@huggingface) <a href="https://twitter.com/huggingface/status/1235583660415488001?ref_src=twsrc%5Etfw">March 5, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



